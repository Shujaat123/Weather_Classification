{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weather_Classification_Notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/Weather_Classification/blob/master/Weather_Classification_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEJa2_TZnec"
      },
      "source": [
        "## **WC-LSE: Weather Classification From Natural Color Images Using Deep Latent Space Encoding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-m_byicZkQc"
      },
      "source": [
        "This code provide python implementation of WC-LSE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadaflfshqgF"
      },
      "source": [
        "# Loading Useful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6012nzZktl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dff4be-fd9f-473e-db33-29ff63a74f37"
      },
      "source": [
        "## Load useful packages\n",
        "!pip install wget\n",
        "from random import sample\n",
        "\n",
        "import keras\n",
        "# import os.path\n",
        "from os import path\n",
        "import h5py\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import wget\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIp3d0U-RHC"
      },
      "source": [
        "# Pre-processing wheather-classification dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB-LjEPU97rw"
      },
      "source": [
        "**Loading and processing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK4g9LZ0L9Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4942e295-d898-4431-99dc-df440456e9fa"
      },
      "source": [
        "classes = ('cloudy','rain','shine','sunrise')\n",
        "data_path = 'https://raw.githubusercontent.com/Shujaat123/Weather_Classification/master/dataset/'\n",
        "\n",
        "flist = []\n",
        "for fname in classes:\n",
        "  filename = 'WeatherClassificationDB_'+fname+'.mat'\n",
        "  if(path.exists(filename)):\n",
        "    !rm $filename\n",
        "    print('existing file:', filename, ' has been deleted')\n",
        "  print('downloading latest version of file:', filename)\n",
        "  file_path = data_path + filename\n",
        "  wget.download(file_path, filename)\n",
        "  print('DONE')\n",
        "  flist.append(filename)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading latest version of file: WeatherClassificationDB_cloudy.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_rain.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_shine.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_sunrise.mat\n",
            "DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EduFdZD0sDkH"
      },
      "source": [
        "cloudy_imgs = h5py.File(flist[0], 'r')['images']['input']\n",
        "cloudy_labels = h5py.File(flist[0], 'r')['images']['label']\n",
        "\n",
        "rain_imgs = h5py.File(flist[1], 'r')['images']['input']\n",
        "rain_labels = h5py.File(flist[1], 'r')['images']['label']\n",
        "\n",
        "shine_imgs = h5py.File(flist[2], 'r')['images']['input']\n",
        "shine_labels = h5py.File(flist[2], 'r')['images']['label']\n",
        "\n",
        "sunrise_imgs = h5py.File(flist[3], 'r')['images']['input']\n",
        "sunrise_labels = h5py.File(flist[3], 'r')['images']['label']\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h04gGVW6xbvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d567daf-cadc-4e32-e634-dabaf51348ef"
      },
      "source": [
        "print(cloudy_imgs.shape) # (NHWC)\n",
        "print(cloudy_labels.shape)\n",
        "\n",
        "print(rain_imgs.shape) # (NHWC)\n",
        "print(rain_labels.shape)\n",
        "\n",
        "print(shine_imgs.shape) # (NHWC)\n",
        "print(shine_labels.shape)\n",
        "\n",
        "print(sunrise_imgs.shape) # (NHWC)\n",
        "print(sunrise_labels.shape)\n",
        "\n",
        "\n",
        "InputImages = np.concatenate((cloudy_imgs,rain_imgs,shine_imgs,sunrise_imgs), axis = 0)\n",
        "InputImages = InputImages/InputImages.max()\n",
        "ClassLabels = np.concatenate((cloudy_labels,rain_labels,shine_labels,sunrise_labels), axis = 0)\n",
        "\n",
        "InputImages.shape\n",
        "ClassLabels.shape\n",
        "# ClassLabels = to_categorical(np.int8(np.squeeze(ClassLabels))-1)\n",
        "ClassLabels = to_categorical(np.squeeze(ClassLabels)-1)\n",
        "ClassLabels.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(207, 256, 256, 3)\n",
            "(207, 1, 1, 1)\n",
            "(215, 256, 256, 3)\n",
            "(215, 1, 1, 1)\n",
            "(253, 256, 256, 3)\n",
            "(253, 1, 1, 1)\n",
            "(357, 256, 256, 3)\n",
            "(357, 1, 1, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1032, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SmbBott1Ln_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4f4ffe-b90d-4afe-cfd9-c501b09db138"
      },
      "source": [
        "cloudy_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==0)).flatten())\n",
        "rain_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==1)).flatten())\n",
        "shine_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==2)).flatten())\n",
        "sunrise_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==3)).flatten())\n",
        "total_list = cloudy_list + rain_list + shine_list + sunrise_list\n",
        "\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_list))\n",
        "print('Number of \\'rain\\' samples:',len(rain_list))\n",
        "print('Number of \\'shine\\' samples:',len(shine_list))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_list))\n",
        "print('Total number of samples:',len(total_list))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 'cloudy' samples: 207\n",
            "Number of 'rain' samples: 215\n",
            "Number of 'shine' samples: 253\n",
            "Number of 'sunrise' samples: 357\n",
            "Total number of samples: 1032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx2y2kYA-IXu"
      },
      "source": [
        "**Generate Training, Validation and Test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIbsrCL2qZl",
        "outputId": "b9a77de2-6497-4d37-bfcb-a93d975313e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## train select 150 cloudy, 150 rain, 150 shine and 150 sunrise samples\n",
        "cloudy_train = sample(cloudy_list, 150)\n",
        "rain_train = sample(rain_list, 150)\n",
        "shine_train = sample(shine_list, 150)\n",
        "sunrise_train = sample(sunrise_list, 150)\n",
        "train_list = cloudy_train + rain_train + shine_train + sunrise_train\n",
        "\n",
        "Input_train = InputImages[train_list]\n",
        "Label_train = ClassLabels[train_list]\n",
        "\n",
        "# valid select 20 cloudy, 20 rain, 20 shine and 20 sunrise samples\n",
        "cloudy_val = sample(set(cloudy_list) - set(cloudy_train), 20)\n",
        "rain_val = sample(set(rain_list) - set(rain_train), 20)\n",
        "shine_val = sample(set(shine_list) - set(shine_train), 20)\n",
        "sunrise_val = sample(set(sunrise_list) - set(sunrise_train), 20)\n",
        "val_list = cloudy_val + rain_val + shine_val + sunrise_val\n",
        "\n",
        "Input_val = InputImages[val_list]\n",
        "Label_val = ClassLabels[val_list]\n",
        "\n",
        "## test\n",
        "test_list = list(set(total_list) - set(train_list) - set(val_list))\n",
        "\n",
        "# test_list\n",
        "Input_test = InputImages[test_list]\n",
        "Label_test = ClassLabels[test_list]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a1d6c6a9506e>:12: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  cloudy_val = sample(set(cloudy_list) - set(cloudy_train), 20)\n",
            "<ipython-input-8-a1d6c6a9506e>:13: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  rain_val = sample(set(rain_list) - set(rain_train), 20)\n",
            "<ipython-input-8-a1d6c6a9506e>:14: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  shine_val = sample(set(shine_list) - set(shine_train), 20)\n",
            "<ipython-input-8-a1d6c6a9506e>:15: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  sunrise_val = sample(set(sunrise_list) - set(sunrise_train), 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EycaHLfyzt9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01715eab-b02e-466a-adc9-aaae0079302e"
      },
      "source": [
        "print(70*'-','\\nTraining Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_train))\n",
        "print('Number of \\'rain\\' samples:',len(rain_train))\n",
        "print('Number of \\'shine\\' samples:',len(shine_train))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_train))\n",
        "print('Total number of samples:',len(train_list))\n",
        "\n",
        "print(70*'-','\\nValidation Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_val))\n",
        "print('Number of \\'rain\\' samples:',len(rain_val))\n",
        "print('Number of \\'shine\\' samples:',len(shine_val))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_val))\n",
        "print('Total number of samples:',len(val_list))\n",
        "\n",
        "print(70*'-','\\nTest Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_list)-len(cloudy_train)-len(cloudy_val))\n",
        "print('Number of \\'rain\\' samples:',len(rain_list)-len(rain_train)-len(rain_val))\n",
        "print('Number of \\'shine\\' samples:',len(shine_list)-len(shine_train)-len(shine_val))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_list)-len(sunrise_train)-len(sunrise_val))\n",
        "print('Total number of samples:',len(test_list))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------- \n",
            "Training Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'cloudy' samples: 150\n",
            "Number of 'rain' samples: 150\n",
            "Number of 'shine' samples: 150\n",
            "Number of 'sunrise' samples: 150\n",
            "Total number of samples: 600\n",
            "---------------------------------------------------------------------- \n",
            "Validation Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'cloudy' samples: 20\n",
            "Number of 'rain' samples: 20\n",
            "Number of 'shine' samples: 20\n",
            "Number of 'sunrise' samples: 20\n",
            "Total number of samples: 80\n",
            "---------------------------------------------------------------------- \n",
            "Test Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'cloudy' samples: 37\n",
            "Number of 'rain' samples: 45\n",
            "Number of 'shine' samples: 83\n",
            "Number of 'sunrise' samples: 187\n",
            "Total number of samples: 352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bx5va5XqCD"
      },
      "source": [
        "**Define loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YphC6Dpf_aO8"
      },
      "source": [
        "def loss_DSSIM(y_true, y_pred):\n",
        "    ssim =tf.image.ssim(y_true, y_pred, 1.0)\n",
        "    # tv = tf.image.total_variation(y_true - y_pred)\n",
        "    mae = K.mean(tf.keras.losses.mean_absolute_error(y_true, y_pred))\n",
        "    return K.mean((1.0 - ssim + mae))\n",
        "\n",
        "def DSSIM(y_true, y_pred):\n",
        "    ssim =tf.image.ssim(y_true, y_pred, 1.0)\n",
        "    return K.mean(ssim)\n",
        "\n",
        "def DPSNR(y_true, y_pred):\n",
        "    psnr =tf.image.psnr(y_true, y_pred,1)\n",
        "    return K.mean(psnr)\n",
        "\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth_ = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth_) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth_)\n",
        "    return score\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "    # loss = 1 - dsc(y_true, y_pred)\n",
        "    # return loss\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "  denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
        "\n",
        "  return 1 - (numerator + 1) / (denominator + 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbacT_QSs2Yu"
      },
      "source": [
        "**Define performance measures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIMP-uLzFaT"
      },
      "source": [
        "def pmeasure(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true.ravel(), y_pred.ravel())\n",
        "  tp_0 = cm[0][0]\n",
        "  tp_1 = cm[1][1]\n",
        "  tp_2 = cm[2][2]\n",
        "  tp_3 = cm[3][3]\n",
        "\n",
        "  tn_0 = cm[1][1] + cm[1][2] + cm[1][3] + cm[2][1] + cm[2][2] + cm[2][3] + cm[3][1] + cm[3][2] + cm[3][3]\n",
        "  tn_1 = cm[0][0] + cm[0][2] + cm[0][3] + cm[2][0] + cm[2][2] + cm[2][3] + cm[3][0] + cm[3][2] + cm[3][3]\n",
        "  tn_2 = cm[0][0] + cm[0][1] + cm[0][3] + cm[1][0] + cm[1][1] + cm[1][3] + cm[3][0] + cm[3][1] + cm[3][3]\n",
        "  tn_3 = cm[0][0] + cm[0][1] + cm[0][2] + cm[1][0] + cm[1][1] + cm[1][2] + cm[2][0] + cm[2][1] + cm[2][2]\n",
        "\n",
        "  fp_0 = cm[1][0] + cm[2][0] + cm[3][0]\n",
        "  fp_1 = cm[0][1] + cm[2][1] + cm[3][1]\n",
        "  fp_2 = cm[0][2] + cm[1][2] + cm[3][2]\n",
        "  fp_3 = cm[0][3] + cm[1][3] + cm[2][3]\n",
        "\n",
        "  fn_0 = cm[0][1] + cm[0][2] + cm[0][3]\n",
        "  fn_1 = cm[1][0] + cm[1][2] + cm[1][3]\n",
        "  fn_2 = cm[2][0] + cm[2][1] + cm[2][3]\n",
        "  fn_3 = cm[3][0] + cm[3][1] + cm[3][2]\n",
        "\n",
        "  sensitivity_0 = tp_0 / (tp_0 + fn_0)\n",
        "  sensitivity_1 = tp_1 / (tp_1 + fn_1)\n",
        "  sensitivity_2 = tp_2 / (tp_2 + fn_2)\n",
        "  sensitivity_3 = tp_3 / (tp_3 + fn_3)\n",
        "\n",
        "  specificity_0 = tn_0 / (tn_0 + fp_0)\n",
        "  specificity_1 = tn_1 / (tn_1 + fp_1)\n",
        "  specificity_2 = tn_2 / (tn_2 + fp_2)\n",
        "  specificity_3 = tn_3 / (tn_3 + fp_3)\n",
        "\n",
        "  f1_score_0 = 2 * tp_0 / (2 * (tp_0 + fp_0 + fn_0))\n",
        "  f1_score_1 = 2 * tp_1 / (2 * (tp_1 + fp_1 + fn_1))\n",
        "  f1_score_2 = 2 * tp_2 / (2 * (tp_2 + fp_2 + fn_2))\n",
        "  f1_score_3 = 2 * tp_3 / (2 * (tp_3 + fp_3 + fn_3))\n",
        "\n",
        "\n",
        "  return ({'Cloudy Sensitivity': sensitivity_0, 'Rain Sensitivity': sensitivity_1,\n",
        "           'Shine Sensitivity': sensitivity_2,'Sunrise Sensitivity': sensitivity_3,\n",
        "           'Cloudy Specificity': specificity_0, 'Rain Specificity': specificity_1,\n",
        "           'Shine Specificity': specificity_2,'Sunrise Specificity': specificity_3,\n",
        "           'Cloudy F1-Score': f1_score_0, 'Rain F1-Score': f1_score_1,\n",
        "           'Shine F1-Score': f1_score_1,'Sunrise F1-Score': f1_score_3})\n",
        "\n",
        "def check_preds(ypred, ytrue):\n",
        "    smooth = 1\n",
        "    pred = np.ndarray.flatten(np.clip(ypred, 0, 1))\n",
        "    gt = np.ndarray.flatten(np.clip(ytrue, 0, 1))\n",
        "    intersection = np.sum(pred * gt)\n",
        "    union = np.sum(pred) + np.sum(gt)\n",
        "    return np.round((2 * intersection + smooth)/(union + smooth), decimals=5)\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    smooth = 1\n",
        "    y_pred_pos = np.round(np.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "    y_pos = np.round(np.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        "    tp = np.sum(y_pos * y_pred_pos)\n",
        "    tn = np.sum(y_neg * y_pred_neg)\n",
        "    fp = np.sum(y_neg * y_pred_pos)\n",
        "    fn = np.sum(y_pos * y_pred_neg)\n",
        "    tpr = (tp + smooth) / (tp + fn + smooth)  # recall\n",
        "    tnr = (tn + smooth) / (tn + fp + smooth)\n",
        "    prec = (tp + smooth) / (tp + fp + smooth)  # precision\n",
        "    return [tpr, tnr, prec]\n",
        "\n",
        "def dice_score(y_true, y_pred, thres):\n",
        "  dice = np.zeros(y_true.shape[0])\n",
        "  dsc_thres = np.zeros_like(dice)\n",
        "  recall = np.zeros_like(dice)\n",
        "  precision = np.zeros_like(dice)\n",
        "\n",
        "  for i in range(y_true.shape[0]):\n",
        "    ds = dsc(y_true[i], y_pred[i])\n",
        "    dice[i] = K.eval(ds)\n",
        "    dsc_thres[i] = check_preds(y_pred[i] > thres, y_true[i])\n",
        "    recall[i], _, precision[i] = auc(y_true[i], y_pred[i] > thres)\n",
        "\n",
        "  dice = np.mean(dice)\n",
        "  dsc_thres = np.mean(dsc_thres)\n",
        "  recall = np.mean(recall)\n",
        "  precision = np.mean(precision)\n",
        "\n",
        "  return {'Dice score': dice, 'Dice score with threshold': dsc_thres, 'Recall': recall, 'Precision': precision}\n",
        "\n",
        "def Show_Statistics(msg, Stat):\n",
        "    print(msg.upper())\n",
        "    print(70 * '-')\n",
        "    print('Accuracy:', Stat[0])\n",
        "\n",
        "    print('Cloudy Sensitivity:', Stat[1])\n",
        "    print('Rain Sensitivity:', Stat[2])\n",
        "    print('Shine Sensitivity:', Stat[3])\n",
        "    print('Sunrise Sensitivity:', Stat[4])\n",
        "\n",
        "    print('Cloudy Specificity:', Stat[5])\n",
        "    print('Rain Specificity:', Stat[6])\n",
        "    print('Shine Specificity:', Stat[7])\n",
        "    print('Sunrise Specificity:', Stat[8])\n",
        "\n",
        "    print('Cloudy F1-Score:', Stat[9])\n",
        "    print('Rain F1-Score:', Stat[10])\n",
        "    print('Shine F1-Score:', Stat[11])\n",
        "    print('Sunrise F1-Score:', Stat[12])\n",
        "\n",
        "    print('Balance Accuracy:', Stat[13])\n",
        "    print(70 * '-')\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9mtHsEaAyA"
      },
      "source": [
        "# Designing an Auto-Encoder-based classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge7uF0idR-Gc"
      },
      "source": [
        "# def BUS_Final_Model(num_filters=64,input_shape=(256, 256, 3)):\n",
        "#     # Encoder Network\n",
        "#     encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "#     enc_l1 = Conv2D(num_filters, 15, activation='relu', name='encoder_layer1', padding='same')(encoder_input)\n",
        "#     enc_l1 = BatchNormalization()(enc_l1)\n",
        "#     enc_l1 = Conv2D(num_filters, 3, activation='relu', name='encoder_layer2', padding='same')(enc_l1)\n",
        "#     enc_l1 = BatchNormalization()(enc_l1)\n",
        "#     # enc_l1 = Dropout(0.4)(enc_l1)\n",
        "#     enc_l1 = MaxPooling2D(pool_size=(2, 2))(enc_l1)\n",
        "\n",
        "#     enc_l2 = Conv2D(2 * num_filters, 3, activation='relu', name='encoder_layer3', padding='same')(enc_l1)\n",
        "#     enc_l2 = BatchNormalization()(enc_l2)\n",
        "#     enc_l2 = Conv2D(2 * num_filters, 3, activation='relu', name='encoder_layer4', padding='same')(enc_l2)\n",
        "#     enc_l2 = BatchNormalization()(enc_l2)\n",
        "#     # enc_l2 = Dropout(0.4)(enc_l2)\n",
        "#     enc_l2 = MaxPooling2D(pool_size=(2, 2))(enc_l2)\n",
        "\n",
        "#     enc_l3 = Conv2D(4 * num_filters, 3, activation='relu', name='encoder_layer5', padding='same')(enc_l2)\n",
        "#     enc_l3 = BatchNormalization()(enc_l3)\n",
        "#     enc_l3 = Conv2D(4 * num_filters, 3, activation='relu', name='encoder_layer6', padding='same')(enc_l3)\n",
        "#     enc_l3 = BatchNormalization()(enc_l3)\n",
        "#     # enc_l3 = Dropout(0.4)(enc_l3)\n",
        "#     enc_l3 = MaxPooling2D(pool_size=(2, 2))(enc_l3)\n",
        "\n",
        "#     enc_l4 = Conv2D(8 * num_filters, 3, activation='relu', name='encoder_layer7', padding='same')(enc_l3)\n",
        "#     enc_l4 = BatchNormalization()(enc_l4)\n",
        "#     enc_l4 = Conv2D(8 * num_filters, 3, activation='relu', name='encoder_layer8', padding='same')(enc_l4)\n",
        "#     enc_l4 = BatchNormalization()(enc_l4)\n",
        "#     # enc_l4 = Dropout(0.4)(enc_l4)\n",
        "#     enc_l4 = MaxPooling2D(pool_size=(2, 2))(enc_l4)\n",
        "\n",
        "#     enc_l5 = Conv2D(16 * num_filters, 3, activation='relu', name='encoder_layer9', padding='same')(enc_l4)\n",
        "#     enc_l5 = BatchNormalization()(enc_l5)\n",
        "#     enc_l5 = Conv2D(16 * num_filters, 3, activation='relu', name='encoder_layer10', padding='same')(enc_l5)\n",
        "#     enc_l5 = BatchNormalization()(enc_l5)\n",
        "#     enc_l5 = Dropout(0.4)(enc_l5)\n",
        "#     enc_l5 = MaxPooling2D(pool_size=(2, 2))(enc_l5)\n",
        "\n",
        "#     encoder_output = Conv2D(32 * num_filters, 3, activation='relu', name='encoder_output', padding='same')(enc_l5)\n",
        "\n",
        "#     # Classifier Network\n",
        "#     flat = Flatten()(encoder_output)\n",
        "#     class_l1 = Dense(1024, activation='relu', name='class_layer1')(flat)\n",
        "#     class_l1 = BatchNormalization()(class_l1)\n",
        "#     drop_c1 = Dropout(0.4)(class_l1)\n",
        "#     class_l2 = Dense(512, activation='relu', name='class_layer2')(drop_c1)\n",
        "#     class_l2 = BatchNormalization()(class_l2)\n",
        "#     drop_c2 = Dropout(0.4)(class_l2)\n",
        "#     class_l3 = Dense(256, activation='relu', name='class_layer3')(drop_c2)\n",
        "#     class_l3 = BatchNormalization()(class_l3)\n",
        "#     drop_c3 = Dropout(0.4)(class_l3)\n",
        "#     class_output = Dense(4, activation='softmax', name='class_output')(drop_c3)\n",
        "\n",
        "#     # Decoder Network\n",
        "#     # dec_l1 = UpSampling2D(size=(2, 2))(concatenate([enc_l5, encoder_output], axis=3))\n",
        "#     dec_l1 = UpSampling2D(size=(2, 2))(encoder_output)\n",
        "#     dec_l1 = Conv2D(16 * num_filters, 3, activation='relu', name='decoder_layer1', padding='same')(dec_l1)\n",
        "#     dec_l1 = BatchNormalization()(dec_l1)\n",
        "#     dec_l1 = Conv2D(16 * num_filters, 3, activation='relu', name='decoder_layer2', padding='same')(dec_l1)\n",
        "#     dec_l1 = BatchNormalization()(dec_l1)\n",
        "#     # dec_l1 = Dropout(0.4)(dec_l1)\n",
        "\n",
        "#     # dec_l2 = UpSampling2D(size=(2, 2))(concatenate([enc_l4, dec_l1], axis=3))\n",
        "#     dec_l2 = UpSampling2D(size=(2, 2))(dec_l1)\n",
        "#     dec_l2 = Conv2D(8 * num_filters, 3, activation='relu', name='decoder_layer3', padding='same')(dec_l2)\n",
        "#     # dec_l2 = BatchNormalization()(dec_l2)\n",
        "#     dec_l2 = Conv2D(8 * num_filters, 3, activation='relu', name='decoder_layer4', padding='same')(dec_l2)\n",
        "#     dec_l2 = BatchNormalization()(dec_l2)\n",
        "#     # dec_l2 = Dropout(0.4)(dec_l2)\n",
        "\n",
        "#     # dec_l3 = UpSampling2D(size=(2, 2))(concatenate([enc_l3, dec_l2], axis=3))\n",
        "#     dec_l3 = UpSampling2D(size=(2, 2))(dec_l2)\n",
        "#     dec_l3 = Conv2D(2 * num_filters, 3, activation='relu', name='decoder_layer5', padding='same')(dec_l3)\n",
        "#     # dec_l3 = BatchNormalization()(dec_l3)\n",
        "#     dec_l3 = Conv2D(2 * num_filters, 3, activation='relu', name='decoder_layer6', padding='same')(dec_l3)\n",
        "#     dec_l3 = BatchNormalization()(dec_l3)\n",
        "#     # dec_l3 = Dropout(0.4)(dec_l3)\n",
        "\n",
        "#     # dec_l4 = UpSampling2D(size=(2, 2))(concatenate([enc_l2, dec_l3], axis=3))\n",
        "#     dec_l4 = UpSampling2D(size=(2, 2))(dec_l3)\n",
        "#     dec_l4 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer7', padding='same')(dec_l4)\n",
        "#     # dec_l4 = BatchNormalization()(dec_l4)\n",
        "#     dec_l4 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer8', padding='same')(dec_l4)\n",
        "#     dec_l4 = BatchNormalization()(dec_l4)\n",
        "#     # dec_l4 = Dropout(0.4)(dec_l4)\n",
        "\n",
        "#     # dec_l5 = UpSampling2D(size=(2, 2))(concatenate([enc_l1, dec_l4], axis=3))\n",
        "#     dec_l5 = UpSampling2D(size=(2, 2))(dec_l4)\n",
        "#     dec_l5 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer10', padding='same')(dec_l5)\n",
        "#     # dec_l5 = BatchNormalization()(dec_l5)\n",
        "#     dec_l5 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer9', padding='same')(dec_l5)\n",
        "#     dec_l5 = BatchNormalization()(dec_l5)\n",
        "#     dec_l5 = Dropout(0.4)(dec_l5)\n",
        "\n",
        "#     decoder_output = Conv2D(input_shape[2], 1, activation='relu', name='decoder_output', padding='same')(dec_l5)\n",
        "\n",
        "#     model = Model(inputs=[encoder_input], outputs=[class_output, decoder_output])  # class_output, decoder_output,\n",
        "\n",
        "#     # Compiling model\n",
        "#     model.compile(optimizer=tf.keras.optimizers.Adam(),#learning_rate=0.0001, beta_1=0.9, beta_2=0.999,\n",
        "#                                                       #epsilon=1e-07, amsgrad=False),\n",
        "#                   loss= {'class_output': 'categorical_crossentropy', 'decoder_output': 'mae'}, #loss_DSSIM},\n",
        "#                   loss_weights={'class_output': 0.01,  'decoder_output': 0.99},\n",
        "#                   metrics={'class_output': 'categorical_accuracy',  'decoder_output': 'mae'})#[DSSIM, DPSNR, 'mae']})\n",
        "#     return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyElFX950QUG"
      },
      "source": [
        "def BUS_Final_Model(input_shape=(256, 256, 3)):\n",
        "  base_model = keras.applications.Xception(\n",
        "      weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "      # weights=None,  # Load weights pre-trained on ImageNet.\n",
        "      input_shape=input_shape,\n",
        "      include_top=False)  # Do not include the ImageNet classifier at the top.\n",
        "  base_model.trainable = False\n",
        "  # Encoder Network\n",
        "  encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "  # We make sure that the base_model is running in inference mode here,\n",
        "  # by passing `training=False`. This is important for fine-tuning, as you will\n",
        "  # learn in a few paragraphs.\n",
        "  x = base_model(encoder_input, training=False)\n",
        "  # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "  # A Dense classifier with a single unit (binary classification)\n",
        "  class_output = keras.layers.Dense(4, name='class_output')(x)\n",
        "  model = keras.Model(encoder_input, class_output)\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(),#lr=0.0001),\n",
        "                loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "                # metrics=[keras.metrics.BinaryAccuracy()])\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8UEHjUVOo9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "21cd6fbc-54c9-443b-83be-010b783dfacd"
      },
      "source": [
        "# model = LSE_model(input_size=Bmode_train.shape[1:],filters=8,kernel_size=3,upconv=False,droprate=0.4, batchnorm=2)\n",
        "model = BUS_Final_Model()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ xception (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m20,861,480\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ class_output (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m8,196\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ class_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,196</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,869,676\u001b[0m (79.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,869,676</span> (79.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,196\u001b[0m (32.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,196</span> (32.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctiZpwdn4V4i"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSvqpQsOQyb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "500be60e-41bb-4567-e125-241ac82d2b9e"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "checkpoint = ModelCheckpoint('models\\\\model-best.keras', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "# es = EarlyStopping(monitor='val_class_output_categorical_accuracy', mode='max', verbose=0, patience=10)\n",
        "\n",
        "# checkpoint = ModelCheckpoint('models\\\\model-best.h5', verbose=1,\n",
        "#                              monitor='val_class_output_categorical_accuracy',\n",
        "#                              save_best_only=True, mode='auto')\n",
        "\n",
        "# history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train,'decoder_output': Input_train},\n",
        "#                     validation_data=({'encoder_input': Input_val}, {'class_output': Label_val, 'decoder_output': Input_val}),\n",
        "#                     batch_size=10, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n",
        "\n",
        "history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train},\n",
        "                    validation_data=({'encoder_input': Input_val}, {'class_output': Label_val}),\n",
        "                    batch_size=1, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.36099, saving model to models\\model-best.keras\n",
            "600/600 - 34s - 56ms/step - accuracy: 0.7917 - loss: 0.6090 - val_accuracy: 0.8500 - val_loss: 0.3610\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 2: val_loss improved from 0.36099 to 0.28693, saving model to models\\model-best.keras\n",
            "600/600 - 26s - 43ms/step - accuracy: 0.9100 - loss: 0.2798 - val_accuracy: 0.8625 - val_loss: 0.2869\n",
            "Epoch 3/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ce31ea2cea4c>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#                     batch_size=10, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train},\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'encoder_input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInput_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLabel_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     batch_size=1, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VfQ8rze4ltS"
      },
      "source": [
        "# Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgx7ywtgQdR4"
      },
      "source": [
        "del model\n",
        "model = load_model('models\\model-best.keras')\n",
        "# [Label_train_pred, decoder_output_train_pred] = model.predict(Input_train, batch_size=1, verbose=0)\n",
        "# [Label_val_pred, decoder_output_val_pred] = model.predict(Input_val, batch_size=1, verbose=0)\n",
        "# [Label_test_pred, decoder_output_test_pred] = model.predict(Input_test, batch_size=1, verbose=0)\n",
        "Label_train_pred = model.predict(Input_train, batch_size=1, verbose=0)\n",
        "Label_val_pred = model.predict(Input_val, batch_size=1, verbose=0)\n",
        "Label_test_pred = model.predict(Input_test, batch_size=1, verbose=0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6pOqLuSoOf0"
      },
      "source": [
        "Training_Stats = []\n",
        "# Training Measures\n",
        "tr_acc = accuracy_score(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))\n",
        "tr_sen_a = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Cloudy Sensitivity']\n",
        "tr_sen_b = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Rain Sensitivity']\n",
        "tr_sen_c = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Shine Sensitivity']\n",
        "tr_sen_d = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sunrise Sensitivity']\n",
        "\n",
        "tr_spe_a = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Cloudy Specificity']\n",
        "tr_spe_b = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Rain Specificity']\n",
        "tr_spe_c = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Shine Specificity']\n",
        "tr_spe_d = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sunrise Specificity']\n",
        "\n",
        "tr_f1_a = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Cloudy F1-Score']\n",
        "tr_f1_b = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Rain F1-Score']\n",
        "tr_f1_c = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Shine F1-Score']\n",
        "tr_f1_d = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sunrise F1-Score']\n",
        "tr_bacc = balanced_accuracy_score(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))\n",
        "\n",
        "Training_Stats.append([tr_acc,\n",
        "                       tr_sen_a, tr_sen_b, tr_sen_c, tr_sen_d,\n",
        "                       tr_spe_a, tr_spe_b, tr_spe_c, tr_spe_d,\n",
        "                       tr_f1_a, tr_f1_b, tr_f1_c, tr_f1_d,\n",
        "                       tr_bacc])\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nMkb1OAAjqa"
      },
      "source": [
        "Validation_Stats = []\n",
        "# Validation Measures\n",
        "\n",
        "v_acc = accuracy_score(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))\n",
        "v_sen_a = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Cloudy Sensitivity']\n",
        "v_sen_b = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Rain Sensitivity']\n",
        "v_sen_c = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Shine Sensitivity']\n",
        "v_sen_d = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sunrise Sensitivity']\n",
        "\n",
        "v_spe_a = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Cloudy Specificity']\n",
        "v_spe_b = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Rain Specificity']\n",
        "v_spe_c = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Shine Specificity']\n",
        "v_spe_d = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sunrise Specificity']\n",
        "\n",
        "v_f1_a = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Cloudy F1-Score']\n",
        "v_f1_b = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Rain F1-Score']\n",
        "v_f1_c = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Shine F1-Score']\n",
        "v_f1_d = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sunrise F1-Score']\n",
        "v_bacc = balanced_accuracy_score(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))\n",
        "\n",
        "Validation_Stats.append([v_acc,\n",
        "                  v_sen_a, v_sen_b, v_sen_c, v_sen_d,\n",
        "                  v_spe_a, v_spe_b, v_spe_c, v_spe_d,\n",
        "                  v_f1_a, v_f1_b, v_f1_c, v_f1_d,\n",
        "                  v_bacc])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrRMPjLIA96T"
      },
      "source": [
        "Testing_Stats = []\n",
        "# Test Measures\n",
        "t_acc = accuracy_score(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))\n",
        "t_sen_a = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Cloudy Sensitivity']\n",
        "t_sen_b = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Rain Sensitivity']\n",
        "t_sen_c = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Shine Sensitivity']\n",
        "t_sen_d = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sunrise Sensitivity']\n",
        "\n",
        "t_spe_a = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Cloudy Specificity']\n",
        "t_spe_b = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Rain Specificity']\n",
        "t_spe_c = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Shine Specificity']\n",
        "t_spe_d = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sunrise Specificity']\n",
        "\n",
        "t_f1_a = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Cloudy F1-Score']\n",
        "t_f1_b = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Rain F1-Score']\n",
        "t_f1_c = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Shine F1-Score']\n",
        "t_f1_d = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sunrise F1-Score']\n",
        "t_bacc = balanced_accuracy_score(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))\n",
        "\n",
        "Testing_Stats.append([t_acc,\n",
        "                      t_sen_a, t_sen_b, t_sen_c, t_sen_d,\n",
        "                      t_spe_a, t_spe_b, t_spe_c, t_spe_d,\n",
        "                      t_f1_a, t_f1_b, t_f1_c, t_f1_d,\n",
        "                      t_bacc])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPTontB8qOt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68afcb21-255d-4b9a-cfde-938391a2ec26"
      },
      "source": [
        "Train_Statistics = np.asarray(Training_Stats)\n",
        "Val_Statistics = np.asarray(Validation_Stats)\n",
        "Test_Statistics = np.asarray(Testing_Stats)\n",
        "\n",
        "# Show Classification/Reconstruction Statistics\n",
        "Show_Statistics('Training Results', Train_Statistics[0])\n",
        "Show_Statistics('Validation Results', Val_Statistics[0])\n",
        "Show_Statistics('Test Results', Test_Statistics[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.9366666666666666\n",
            "Cloudy Sensitivity: 0.8133333333333334\n",
            "Rain Sensitivity: 0.98\n",
            "Shine Sensitivity: 0.98\n",
            "Sunrise Sensitivity: 0.9733333333333334\n",
            "Cloudy Specificity: 0.9888888888888889\n",
            "Rain Specificity: 1.0\n",
            "Shine Specificity: 0.9377777777777778\n",
            "Sunrise Specificity: 0.9888888888888889\n",
            "Cloudy F1-Score: 0.7870967741935484\n",
            "Rain F1-Score: 0.98\n",
            "Shine F1-Score: 0.98\n",
            "Sunrise F1-Score: 0.9419354838709677\n",
            "Balance Accuracy: 0.9366666666666668\n",
            "----------------------------------------------------------------------\n",
            "VALIDATION RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.8625\n",
            "Cloudy Sensitivity: 0.65\n",
            "Rain Sensitivity: 1.0\n",
            "Shine Sensitivity: 0.9\n",
            "Sunrise Sensitivity: 0.9\n",
            "Cloudy Specificity: 0.9666666666666667\n",
            "Rain Specificity: 1.0\n",
            "Shine Specificity: 0.8666666666666667\n",
            "Sunrise Specificity: 0.9833333333333333\n",
            "Cloudy F1-Score: 0.5909090909090909\n",
            "Rain F1-Score: 1.0\n",
            "Shine F1-Score: 1.0\n",
            "Sunrise F1-Score: 0.8571428571428571\n",
            "Balance Accuracy: 0.8624999999999999\n",
            "----------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.9460227272727273\n",
            "Cloudy Sensitivity: 0.8108108108108109\n",
            "Rain Sensitivity: 0.9555555555555556\n",
            "Shine Sensitivity: 0.963855421686747\n",
            "Sunrise Sensitivity: 0.9625668449197861\n",
            "Cloudy Specificity: 0.9936507936507937\n",
            "Rain Specificity: 0.993485342019544\n",
            "Shine Specificity: 0.9591078066914498\n",
            "Sunrise Specificity: 0.9757575757575757\n",
            "Cloudy F1-Score: 0.7692307692307693\n",
            "Rain F1-Score: 0.9148936170212766\n",
            "Shine F1-Score: 0.9148936170212766\n",
            "Sunrise F1-Score: 0.9424083769633508\n",
            "Balance Accuracy: 0.9231971582432249\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}