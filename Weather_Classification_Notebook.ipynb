{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weather_Classification_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/Weather_Classification/blob/master/Weather_Classification_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEJa2_TZnec"
      },
      "source": [
        "## **WC-LSE: Weather Classification From Natural Color Images Using Deep Latent Space Encoding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-m_byicZkQc"
      },
      "source": [
        "This code provide python implementation of WC-LSE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadaflfshqgF"
      },
      "source": [
        "# Loading Useful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6012nzZktl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d412d31-6f0c-4ae3-9c1e-fb6935b03bdb"
      },
      "source": [
        "## Load useful packages\n",
        "!pip install wget\n",
        "from random import sample\n",
        "\n",
        "import keras\n",
        "# import os.path\n",
        "from os import path\n",
        "import h5py\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import wget\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIp3d0U-RHC"
      },
      "source": [
        "# Pre-processing wheather-classification dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB-LjEPU97rw"
      },
      "source": [
        "**Loading and processing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK4g9LZ0L9Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cad0df-b3e2-4c12-c056-12f3bdbae005"
      },
      "source": [
        "classes = ('cloudy','rain','shine','sunrise')\n",
        "data_path = 'https://raw.githubusercontent.com/Shujaat123/Weather_Classification/master/dataset/'\n",
        "\n",
        "flist = []\n",
        "for fname in classes:\n",
        "  filename = 'WeatherClassificationDB_'+fname+'.mat'\n",
        "  if(path.exists(filename)):\n",
        "    !rm $filename\n",
        "    print('existing file:', filename, ' has been deleted')\n",
        "  print('downloading latest version of file:', filename)\n",
        "  file_path = data_path + filename\n",
        "  wget.download(file_path, filename)\n",
        "  print('DONE')\n",
        "  flist.append(filename)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading latest version of file: WeatherClassificationDB_cloudy.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_rain.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_shine.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_sunrise.mat\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EduFdZD0sDkH"
      },
      "source": [
        "cloudy_imgs = h5py.File(flist[0], 'r')['images']['input']\n",
        "cloudy_labels = h5py.File(flist[0], 'r')['images']['label']\n",
        "\n",
        "rain_imgs = h5py.File(flist[1], 'r')['images']['input']\n",
        "rain_labels = h5py.File(flist[1], 'r')['images']['label']\n",
        "\n",
        "shine_imgs = h5py.File(flist[2], 'r')['images']['input']\n",
        "shine_labels = h5py.File(flist[2], 'r')['images']['label']\n",
        "\n",
        "sunrise_imgs = h5py.File(flist[3], 'r')['images']['input']\n",
        "sunrise_labels = h5py.File(flist[3], 'r')['images']['label']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h04gGVW6xbvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c996400d-d064-4d22-c11d-5637210ba530"
      },
      "source": [
        "print(cloudy_imgs.shape) # (NHWC)\n",
        "print(cloudy_labels.shape)\n",
        "\n",
        "print(rain_imgs.shape) # (NHWC)\n",
        "print(rain_labels.shape)\n",
        "\n",
        "print(shine_imgs.shape) # (NHWC)\n",
        "print(shine_labels.shape)\n",
        "\n",
        "print(sunrise_imgs.shape) # (NHWC)\n",
        "print(sunrise_labels.shape)\n",
        "\n",
        "\n",
        "InputImages = np.concatenate((cloudy_imgs,rain_imgs,shine_imgs,sunrise_imgs), axis = 0)\n",
        "InputImages = InputImages/InputImages.max()\n",
        "ClassLabels = np.concatenate((cloudy_labels,rain_labels,shine_labels,sunrise_labels), axis = 0)\n",
        "\n",
        "InputImages.shape\n",
        "ClassLabels.shape\n",
        "# ClassLabels = to_categorical(np.int8(np.squeeze(ClassLabels))-1)\n",
        "ClassLabels = to_categorical(np.squeeze(ClassLabels)-1)\n",
        "ClassLabels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(207, 256, 256, 3)\n",
            "(207, 1, 1, 1)\n",
            "(215, 256, 256, 3)\n",
            "(215, 1, 1, 1)\n",
            "(253, 256, 256, 3)\n",
            "(253, 1, 1, 1)\n",
            "(357, 256, 256, 3)\n",
            "(357, 1, 1, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1032, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SmbBott1Ln_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8f4c54-96c9-436c-c01f-6f8b7bf4502a"
      },
      "source": [
        "cloudy_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==0)).flatten())\n",
        "rain_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==1)).flatten())\n",
        "shine_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==2)).flatten())\n",
        "sunrise_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==3)).flatten())\n",
        "total_list = cloudy_list + rain_list + shine_list + sunrise_list\n",
        "\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_list))\n",
        "print('Number of \\'rain\\' samples:',len(rain_list))\n",
        "print('Number of \\'shine\\' samples:',len(shine_list))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_list))\n",
        "print('Total number of samples:',len(total_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of 'cloudy' samples: 207\n",
            "Number of 'rain' samples: 215\n",
            "Number of 'shine' samples: 253\n",
            "Number of 'sunrise' samples: 357\n",
            "Total number of samples: 1032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx2y2kYA-IXu"
      },
      "source": [
        "**Generate Training, Validation and Test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIbsrCL2qZl"
      },
      "source": [
        "## train select 150 cloudy, 150 rain, 150 shine and 150 sunrise samples\n",
        "cloudy_train = sample(cloudy_list, 150)\n",
        "rain_train = sample(rain_list, 150)\n",
        "shine_train = sample(shine_list, 150)\n",
        "sunrise_train = sample(sunrise_list, 150)\n",
        "train_list = cloudy_train + rain_train + shine_train + sunrise_train\n",
        "\n",
        "Input_train = InputImages[train_list]\n",
        "Label_train = ClassLabels[train_list]\n",
        "\n",
        "# valid select 20 cloudy, 20 rain, 20 shine and 20 sunrise samples\n",
        "cloudy_val = sample(set(cloudy_list) - set(cloudy_train), 20)\n",
        "rain_val = sample(set(rain_list) - set(rain_train), 20)\n",
        "shine_val = sample(set(shine_list) - set(shine_train), 20)\n",
        "sunrise_val = sample(set(sunrise_list) - set(sunrise_train), 20)\n",
        "val_list = cloudy_val + rain_val + shine_val + sunrise_val\n",
        "\n",
        "Input_val = InputImages[val_list]\n",
        "Label_val = ClassLabels[val_list]\n",
        "\n",
        "## test\n",
        "test_list = list(set(total_list) - set(train_list) - set(val_list))\n",
        "\n",
        "# test_list\n",
        "Input_test = InputImages[test_list]\n",
        "Label_test = ClassLabels[test_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EycaHLfyzt9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7eb6d11-a518-4203-9aaa-33b93681f027"
      },
      "source": [
        "print(70*'-','\\nTraining Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_train))\n",
        "print('Number of \\'rain\\' samples:',len(rain_train))\n",
        "print('Number of \\'shine\\' samples:',len(shine_train))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_train))\n",
        "print('Total number of samples:',len(train_list))\n",
        "\n",
        "print(70*'-','\\nValidation Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_val))\n",
        "print('Number of \\'rain\\' samples:',len(rain_val))\n",
        "print('Number of \\'shine\\' samples:',len(shine_val))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_val))\n",
        "print('Total number of samples:',len(val_list))\n",
        "\n",
        "print(70*'-','\\nTest Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_list)-len(cloudy_train)-len(cloudy_val))\n",
        "print('Number of \\'rain\\' samples:',len(rain_list)-len(rain_train)-len(rain_val))\n",
        "print('Number of \\'shine\\' samples:',len(shine_list)-len(shine_train)-len(shine_val))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_list)-len(sunrise_train)-len(sunrise_val))\n",
        "print('Total number of samples:',len(test_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------- \n",
            "Training Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'cloudy' samples: 150\n",
            "Number of 'rain' samples: 150\n",
            "Number of 'shine' samples: 150\n",
            "Number of 'sunrise' samples: 150\n",
            "Total number of samples: 600\n",
            "---------------------------------------------------------------------- \n",
            "Validation Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'cloudy' samples: 20\n",
            "Number of 'rain' samples: 20\n",
            "Number of 'shine' samples: 20\n",
            "Number of 'sunrise' samples: 20\n",
            "Total number of samples: 80\n",
            "---------------------------------------------------------------------- \n",
            "Test Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'cloudy' samples: 37\n",
            "Number of 'rain' samples: 45\n",
            "Number of 'shine' samples: 83\n",
            "Number of 'sunrise' samples: 187\n",
            "Total number of samples: 352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bx5va5XqCD"
      },
      "source": [
        "**Define loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YphC6Dpf_aO8"
      },
      "source": [
        "def loss_DSSIM(y_true, y_pred):\n",
        "    ssim =tf.image.ssim(y_true, y_pred, 1.0)\n",
        "    # tv = tf.image.total_variation(y_true - y_pred)\n",
        "    mae = K.mean(tf.keras.losses.mean_absolute_error(y_true, y_pred))\n",
        "    return K.mean((1.0 - ssim + mae))\n",
        "\n",
        "def DSSIM(y_true, y_pred):\n",
        "    ssim =tf.image.ssim(y_true, y_pred, 1.0)\n",
        "    return K.mean(ssim)\n",
        "\n",
        "def DPSNR(y_true, y_pred):\n",
        "    psnr =tf.image.psnr(y_true, y_pred,1)\n",
        "    return K.mean(psnr)\n",
        "\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth_ = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth_) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth_)\n",
        "    return score\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "    # loss = 1 - dsc(y_true, y_pred)\n",
        "    # return loss\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "  denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
        "\n",
        "  return 1 - (numerator + 1) / (denominator + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbacT_QSs2Yu"
      },
      "source": [
        "**Define performance measures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIMP-uLzFaT"
      },
      "source": [
        "def pmeasure(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true.ravel(), y_pred.ravel())\n",
        "  tp_0 = cm[0][0]\n",
        "  tp_1 = cm[1][1]\n",
        "  tp_2 = cm[2][2]\n",
        "  tp_3 = cm[3][3]\n",
        "\n",
        "  tn_0 = cm[1][1] + cm[1][2] + cm[1][3] + cm[2][1] + cm[2][2] + cm[2][3] + cm[3][1] + cm[3][2] + cm[3][3]\n",
        "  tn_1 = cm[0][0] + cm[0][2] + cm[0][3] + cm[2][0] + cm[2][2] + cm[2][3] + cm[3][0] + cm[3][2] + cm[3][3]\n",
        "  tn_2 = cm[0][0] + cm[0][1] + cm[0][3] + cm[1][0] + cm[1][1] + cm[1][3] + cm[3][0] + cm[3][1] + cm[3][3]\n",
        "  tn_3 = cm[0][0] + cm[0][1] + cm[0][2] + cm[1][0] + cm[1][1] + cm[1][2] + cm[2][0] + cm[2][1] + cm[2][2]\n",
        "\n",
        "  fp_0 = cm[1][0] + cm[2][0] + cm[3][0]\n",
        "  fp_1 = cm[0][1] + cm[2][1] + cm[3][1]\n",
        "  fp_2 = cm[0][2] + cm[1][2] + cm[3][2]\n",
        "  fp_3 = cm[0][3] + cm[1][3] + cm[2][3]\n",
        "\n",
        "  fn_0 = cm[0][1] + cm[0][2] + cm[0][3]\n",
        "  fn_1 = cm[1][0] + cm[1][2] + cm[1][3]\n",
        "  fn_2 = cm[2][0] + cm[2][1] + cm[2][3]\n",
        "  fn_3 = cm[3][0] + cm[3][1] + cm[3][2]\n",
        "\n",
        "  sensitivity_0 = tp_0 / (tp_0 + fn_0)\n",
        "  sensitivity_1 = tp_1 / (tp_1 + fn_1)\n",
        "  sensitivity_2 = tp_2 / (tp_2 + fn_2)\n",
        "  sensitivity_3 = tp_3 / (tp_3 + fn_3)\n",
        "\n",
        "  specificity_0 = tn_0 / (tn_0 + fp_0)\n",
        "  specificity_1 = tn_1 / (tn_1 + fp_1)\n",
        "  specificity_2 = tn_2 / (tn_2 + fp_2)\n",
        "  specificity_3 = tn_3 / (tn_3 + fp_3)\n",
        "\n",
        "  f1_score_0 = 2 * tp_0 / (2 * (tp_0 + fp_0 + fn_0))\n",
        "  f1_score_1 = 2 * tp_1 / (2 * (tp_1 + fp_1 + fn_1))\n",
        "  f1_score_2 = 2 * tp_2 / (2 * (tp_2 + fp_2 + fn_2))\n",
        "  f1_score_3 = 2 * tp_3 / (2 * (tp_3 + fp_3 + fn_3))\n",
        "\n",
        "\n",
        "  return ({'Cloudy Sensitivity': sensitivity_0, 'Rain Sensitivity': sensitivity_1,\n",
        "           'Shine Sensitivity': sensitivity_2,'Sunrise Sensitivity': sensitivity_3,\n",
        "           'Cloudy Specificity': specificity_0, 'Rain Specificity': specificity_1,\n",
        "           'Shine Specificity': specificity_2,'Sunrise Specificity': specificity_3,\n",
        "           'Cloudy F1-Score': f1_score_0, 'Rain F1-Score': f1_score_1,\n",
        "           'Shine F1-Score': f1_score_1,'Sunrise F1-Score': f1_score_3})\n",
        "  \n",
        "def check_preds(ypred, ytrue):\n",
        "    smooth = 1\n",
        "    pred = np.ndarray.flatten(np.clip(ypred, 0, 1))\n",
        "    gt = np.ndarray.flatten(np.clip(ytrue, 0, 1))\n",
        "    intersection = np.sum(pred * gt) \n",
        "    union = np.sum(pred) + np.sum(gt)   \n",
        "    return np.round((2 * intersection + smooth)/(union + smooth), decimals=5)\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    smooth = 1\n",
        "    y_pred_pos = np.round(np.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "    y_pos = np.round(np.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        "    tp = np.sum(y_pos * y_pred_pos)\n",
        "    tn = np.sum(y_neg * y_pred_neg)\n",
        "    fp = np.sum(y_neg * y_pred_pos)\n",
        "    fn = np.sum(y_pos * y_pred_neg)\n",
        "    tpr = (tp + smooth) / (tp + fn + smooth)  # recall\n",
        "    tnr = (tn + smooth) / (tn + fp + smooth)\n",
        "    prec = (tp + smooth) / (tp + fp + smooth)  # precision\n",
        "    return [tpr, tnr, prec]\n",
        "  \n",
        "def dice_score(y_true, y_pred, thres):\n",
        "  dice = np.zeros(y_true.shape[0])\n",
        "  dsc_thres = np.zeros_like(dice)\n",
        "  recall = np.zeros_like(dice)\n",
        "  precision = np.zeros_like(dice)\n",
        "\n",
        "  for i in range(y_true.shape[0]):\n",
        "    ds = dsc(y_true[i], y_pred[i])\n",
        "    dice[i] = K.eval(ds)\n",
        "    dsc_thres[i] = check_preds(y_pred[i] > thres, y_true[i])\n",
        "    recall[i], _, precision[i] = auc(y_true[i], y_pred[i] > thres)\n",
        "  \n",
        "  dice = np.mean(dice)\n",
        "  dsc_thres = np.mean(dsc_thres)\n",
        "  recall = np.mean(recall)\n",
        "  precision = np.mean(precision)\n",
        "\n",
        "  return {'Dice score': dice, 'Dice score with threshold': dsc_thres, 'Recall': recall, 'Precision': precision}\n",
        "\n",
        "def Show_Statistics(msg, Stat):\n",
        "    print(msg.upper())\n",
        "    print(70 * '-')\n",
        "    print('Accuracy:', Stat[0])\n",
        "\n",
        "    print('Cloudy Sensitivity:', Stat[1])\n",
        "    print('Rain Sensitivity:', Stat[2])\n",
        "    print('Shine Sensitivity:', Stat[3])\n",
        "    print('Sunrise Sensitivity:', Stat[4])\n",
        "\n",
        "    print('Cloudy Specificity:', Stat[5])\n",
        "    print('Rain Specificity:', Stat[6])\n",
        "    print('Shine Specificity:', Stat[7])\n",
        "    print('Sunrise Specificity:', Stat[8])\n",
        "\n",
        "    print('Cloudy F1-Score:', Stat[9])\n",
        "    print('Rain F1-Score:', Stat[10])\n",
        "    print('Shine F1-Score:', Stat[11])\n",
        "    print('Sunrise F1-Score:', Stat[12])\n",
        "\n",
        "    print('Balance Accuracy:', Stat[13])\n",
        "    print(70 * '-')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9mtHsEaAyA"
      },
      "source": [
        "# Designing an Auto-Encoder-based classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge7uF0idR-Gc"
      },
      "source": [
        "# def BUS_Final_Model(num_filters=64,input_shape=(256, 256, 3)):\n",
        "#     # Encoder Network\n",
        "#     encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "#     enc_l1 = Conv2D(num_filters, 15, activation='relu', name='encoder_layer1', padding='same')(encoder_input)\n",
        "#     enc_l1 = BatchNormalization()(enc_l1)\n",
        "#     enc_l1 = Conv2D(num_filters, 3, activation='relu', name='encoder_layer2', padding='same')(enc_l1)\n",
        "#     enc_l1 = BatchNormalization()(enc_l1)\n",
        "#     # enc_l1 = Dropout(0.4)(enc_l1)\n",
        "#     enc_l1 = MaxPooling2D(pool_size=(2, 2))(enc_l1)\n",
        "\n",
        "#     enc_l2 = Conv2D(2 * num_filters, 3, activation='relu', name='encoder_layer3', padding='same')(enc_l1)\n",
        "#     enc_l2 = BatchNormalization()(enc_l2)\n",
        "#     enc_l2 = Conv2D(2 * num_filters, 3, activation='relu', name='encoder_layer4', padding='same')(enc_l2)\n",
        "#     enc_l2 = BatchNormalization()(enc_l2)\n",
        "#     # enc_l2 = Dropout(0.4)(enc_l2)\n",
        "#     enc_l2 = MaxPooling2D(pool_size=(2, 2))(enc_l2)\n",
        "\n",
        "#     enc_l3 = Conv2D(4 * num_filters, 3, activation='relu', name='encoder_layer5', padding='same')(enc_l2)\n",
        "#     enc_l3 = BatchNormalization()(enc_l3)\n",
        "#     enc_l3 = Conv2D(4 * num_filters, 3, activation='relu', name='encoder_layer6', padding='same')(enc_l3)\n",
        "#     enc_l3 = BatchNormalization()(enc_l3)\n",
        "#     # enc_l3 = Dropout(0.4)(enc_l3)\n",
        "#     enc_l3 = MaxPooling2D(pool_size=(2, 2))(enc_l3)\n",
        "\n",
        "#     enc_l4 = Conv2D(8 * num_filters, 3, activation='relu', name='encoder_layer7', padding='same')(enc_l3)\n",
        "#     enc_l4 = BatchNormalization()(enc_l4)\n",
        "#     enc_l4 = Conv2D(8 * num_filters, 3, activation='relu', name='encoder_layer8', padding='same')(enc_l4)\n",
        "#     enc_l4 = BatchNormalization()(enc_l4)\n",
        "#     # enc_l4 = Dropout(0.4)(enc_l4)\n",
        "#     enc_l4 = MaxPooling2D(pool_size=(2, 2))(enc_l4)\n",
        "\n",
        "#     enc_l5 = Conv2D(16 * num_filters, 3, activation='relu', name='encoder_layer9', padding='same')(enc_l4)\n",
        "#     enc_l5 = BatchNormalization()(enc_l5)\n",
        "#     enc_l5 = Conv2D(16 * num_filters, 3, activation='relu', name='encoder_layer10', padding='same')(enc_l5)\n",
        "#     enc_l5 = BatchNormalization()(enc_l5)\n",
        "#     enc_l5 = Dropout(0.4)(enc_l5)\n",
        "#     enc_l5 = MaxPooling2D(pool_size=(2, 2))(enc_l5)\n",
        "\n",
        "#     encoder_output = Conv2D(32 * num_filters, 3, activation='relu', name='encoder_output', padding='same')(enc_l5)\n",
        "\n",
        "#     # Classifier Network\n",
        "#     flat = Flatten()(encoder_output)\n",
        "#     class_l1 = Dense(1024, activation='relu', name='class_layer1')(flat)\n",
        "#     class_l1 = BatchNormalization()(class_l1)\n",
        "#     drop_c1 = Dropout(0.4)(class_l1)\n",
        "#     class_l2 = Dense(512, activation='relu', name='class_layer2')(drop_c1)\n",
        "#     class_l2 = BatchNormalization()(class_l2)\n",
        "#     drop_c2 = Dropout(0.4)(class_l2)\n",
        "#     class_l3 = Dense(256, activation='relu', name='class_layer3')(drop_c2)\n",
        "#     class_l3 = BatchNormalization()(class_l3)\n",
        "#     drop_c3 = Dropout(0.4)(class_l3)\n",
        "#     class_output = Dense(4, activation='softmax', name='class_output')(drop_c3)\n",
        "\n",
        "#     # Decoder Network\n",
        "#     # dec_l1 = UpSampling2D(size=(2, 2))(concatenate([enc_l5, encoder_output], axis=3))\n",
        "#     dec_l1 = UpSampling2D(size=(2, 2))(encoder_output)\n",
        "#     dec_l1 = Conv2D(16 * num_filters, 3, activation='relu', name='decoder_layer1', padding='same')(dec_l1)\n",
        "#     dec_l1 = BatchNormalization()(dec_l1)\n",
        "#     dec_l1 = Conv2D(16 * num_filters, 3, activation='relu', name='decoder_layer2', padding='same')(dec_l1)\n",
        "#     dec_l1 = BatchNormalization()(dec_l1)\n",
        "#     # dec_l1 = Dropout(0.4)(dec_l1)\n",
        "\n",
        "#     # dec_l2 = UpSampling2D(size=(2, 2))(concatenate([enc_l4, dec_l1], axis=3))\n",
        "#     dec_l2 = UpSampling2D(size=(2, 2))(dec_l1)\n",
        "#     dec_l2 = Conv2D(8 * num_filters, 3, activation='relu', name='decoder_layer3', padding='same')(dec_l2)\n",
        "#     # dec_l2 = BatchNormalization()(dec_l2)\n",
        "#     dec_l2 = Conv2D(8 * num_filters, 3, activation='relu', name='decoder_layer4', padding='same')(dec_l2)\n",
        "#     dec_l2 = BatchNormalization()(dec_l2)\n",
        "#     # dec_l2 = Dropout(0.4)(dec_l2)\n",
        "\n",
        "#     # dec_l3 = UpSampling2D(size=(2, 2))(concatenate([enc_l3, dec_l2], axis=3))\n",
        "#     dec_l3 = UpSampling2D(size=(2, 2))(dec_l2)\n",
        "#     dec_l3 = Conv2D(2 * num_filters, 3, activation='relu', name='decoder_layer5', padding='same')(dec_l3)\n",
        "#     # dec_l3 = BatchNormalization()(dec_l3)\n",
        "#     dec_l3 = Conv2D(2 * num_filters, 3, activation='relu', name='decoder_layer6', padding='same')(dec_l3)\n",
        "#     dec_l3 = BatchNormalization()(dec_l3)\n",
        "#     # dec_l3 = Dropout(0.4)(dec_l3)\n",
        "\n",
        "#     # dec_l4 = UpSampling2D(size=(2, 2))(concatenate([enc_l2, dec_l3], axis=3))\n",
        "#     dec_l4 = UpSampling2D(size=(2, 2))(dec_l3)\n",
        "#     dec_l4 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer7', padding='same')(dec_l4)\n",
        "#     # dec_l4 = BatchNormalization()(dec_l4)\n",
        "#     dec_l4 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer8', padding='same')(dec_l4)\n",
        "#     dec_l4 = BatchNormalization()(dec_l4)\n",
        "#     # dec_l4 = Dropout(0.4)(dec_l4)\n",
        "\n",
        "#     # dec_l5 = UpSampling2D(size=(2, 2))(concatenate([enc_l1, dec_l4], axis=3))\n",
        "#     dec_l5 = UpSampling2D(size=(2, 2))(dec_l4)\n",
        "#     dec_l5 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer10', padding='same')(dec_l5)\n",
        "#     # dec_l5 = BatchNormalization()(dec_l5)\n",
        "#     dec_l5 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer9', padding='same')(dec_l5)\n",
        "#     dec_l5 = BatchNormalization()(dec_l5)\n",
        "#     dec_l5 = Dropout(0.4)(dec_l5)\n",
        "\n",
        "#     decoder_output = Conv2D(input_shape[2], 1, activation='relu', name='decoder_output', padding='same')(dec_l5)\n",
        "\n",
        "#     model = Model(inputs=[encoder_input], outputs=[class_output, decoder_output])  # class_output, decoder_output,\n",
        "\n",
        "#     # Compiling model\n",
        "#     model.compile(optimizer=tf.keras.optimizers.Adam(),#learning_rate=0.0001, beta_1=0.9, beta_2=0.999,\n",
        "#                                                       #epsilon=1e-07, amsgrad=False),\n",
        "#                   loss= {'class_output': 'categorical_crossentropy', 'decoder_output': 'mae'}, #loss_DSSIM},\n",
        "#                   loss_weights={'class_output': 0.01,  'decoder_output': 0.99},\n",
        "#                   metrics={'class_output': 'categorical_accuracy',  'decoder_output': 'mae'})#[DSSIM, DPSNR, 'mae']})\n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyElFX950QUG"
      },
      "source": [
        "def BUS_Final_Model(input_shape=(256, 256, 3)):\n",
        "  base_model = keras.applications.Xception(\n",
        "      weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "      # weights=None,  # Load weights pre-trained on ImageNet.\n",
        "      input_shape=input_shape,\n",
        "      include_top=False)  # Do not include the ImageNet classifier at the top.\n",
        "  base_model.trainable = False\n",
        "  # Encoder Network\n",
        "  encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "  # We make sure that the base_model is running in inference mode here,\n",
        "  # by passing `training=False`. This is important for fine-tuning, as you will\n",
        "  # learn in a few paragraphs.\n",
        "  x = base_model(encoder_input, training=False)\n",
        "  # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "  # A Dense classifier with a single unit (binary classification)\n",
        "  class_output = keras.layers.Dense(4, name='class_output')(x)\n",
        "  model = keras.Model(encoder_input, class_output)\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(),#lr=0.0001),\n",
        "                loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "                # metrics=[keras.metrics.BinaryAccuracy()])\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8UEHjUVOo9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0031a94-2b57-4f71-d29f-35dd22f2bbff"
      },
      "source": [
        "# model = LSE_model(input_size=Bmode_train.shape[1:],filters=8,kernel_size=3,upconv=False,droprate=0.4, batchnorm=2)\n",
        "model = BUS_Final_Model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 8, 8, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "class_output (Dense)         (None, 4)                 8196      \n",
            "=================================================================\n",
            "Total params: 20,869,676\n",
            "Trainable params: 8,196\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctiZpwdn4V4i"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSvqpQsOQyb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3c62a7-347f-4856-eee7-f9e1d86418a4"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "checkpoint = ModelCheckpoint('models\\\\model-best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "# es = EarlyStopping(monitor='val_class_output_categorical_accuracy', mode='max', verbose=0, patience=10)\n",
        "\n",
        "# checkpoint = ModelCheckpoint('models\\\\model-best.h5', verbose=1,\n",
        "#                              monitor='val_class_output_categorical_accuracy',\n",
        "#                              save_best_only=True, mode='auto')\n",
        "  \n",
        "# history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train,'decoder_output': Input_train},             \n",
        "#                     validation_data=({'encoder_input': Input_val}, {'class_output': Label_val, 'decoder_output': Input_val}),                                      \n",
        "#                     batch_size=10, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n",
        "\n",
        "history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train},\n",
        "                    validation_data=({'encoder_input': Input_val}, {'class_output': Label_val}),\n",
        "                    batch_size=1, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.40021, saving model to models\\model-best.h5\n",
            "600/600 - 7s - loss: 0.6241 - accuracy: 0.7900 - val_loss: 0.4002 - val_accuracy: 0.9000\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.40021 to 0.30337, saving model to models\\model-best.h5\n",
            "600/600 - 7s - loss: 0.2696 - accuracy: 0.9283 - val_loss: 0.3034 - val_accuracy: 0.9250\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.30337 to 0.28193, saving model to models\\model-best.h5\n",
            "600/600 - 7s - loss: 0.1897 - accuracy: 0.9367 - val_loss: 0.2819 - val_accuracy: 0.9250\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.28193 to 0.24655, saving model to models\\model-best.h5\n",
            "600/600 - 7s - loss: 0.1338 - accuracy: 0.9617 - val_loss: 0.2466 - val_accuracy: 0.9250\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.24655 to 0.21946, saving model to models\\model-best.h5\n",
            "600/600 - 7s - loss: 0.1015 - accuracy: 0.9817 - val_loss: 0.2195 - val_accuracy: 0.9125\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.21946\n",
            "600/600 - 6s - loss: 0.0833 - accuracy: 0.9817 - val_loss: 0.2559 - val_accuracy: 0.9000\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.21946\n",
            "600/600 - 6s - loss: 0.0614 - accuracy: 0.9950 - val_loss: 0.2341 - val_accuracy: 0.9125\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.21946\n",
            "600/600 - 6s - loss: 0.0524 - accuracy: 0.9950 - val_loss: 0.2695 - val_accuracy: 0.9000\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.21946\n",
            "600/600 - 6s - loss: 0.0427 - accuracy: 0.9983 - val_loss: 0.2275 - val_accuracy: 0.9125\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.21946\n",
            "600/600 - 6s - loss: 0.0344 - accuracy: 0.9983 - val_loss: 0.2325 - val_accuracy: 0.9125\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.21946\n",
            "600/600 - 6s - loss: 0.0282 - accuracy: 0.9983 - val_loss: 0.2283 - val_accuracy: 0.9125\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.21946 to 0.19795, saving model to models\\model-best.h5\n",
            "600/600 - 7s - loss: 0.0237 - accuracy: 0.9983 - val_loss: 0.1979 - val_accuracy: 0.9000\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9125\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0177 - accuracy: 0.9983 - val_loss: 0.2239 - val_accuracy: 0.9000\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0152 - accuracy: 0.9983 - val_loss: 0.2916 - val_accuracy: 0.9000\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.2317 - val_accuracy: 0.9000\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2142 - val_accuracy: 0.9000\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2317 - val_accuracy: 0.9125\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9000\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.8875\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9000\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.19795\n",
            "600/600 - 6s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2856 - val_accuracy: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VfQ8rze4ltS"
      },
      "source": [
        "# Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgx7ywtgQdR4"
      },
      "source": [
        "del model\n",
        "model = load_model('models\\model-best.h5')              \n",
        "# [Label_train_pred, decoder_output_train_pred] = model.predict(Input_train, batch_size=1, verbose=0)\n",
        "# [Label_val_pred, decoder_output_val_pred] = model.predict(Input_val, batch_size=1, verbose=0)\n",
        "# [Label_test_pred, decoder_output_test_pred] = model.predict(Input_test, batch_size=1, verbose=0)\n",
        "Label_train_pred = model.predict(Input_train, batch_size=1, verbose=0)\n",
        "Label_val_pred = model.predict(Input_val, batch_size=1, verbose=0)\n",
        "Label_test_pred = model.predict(Input_test, batch_size=1, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6pOqLuSoOf0"
      },
      "source": [
        "Training_Stats = []\n",
        "# Training Measures\n",
        "tr_acc = accuracy_score(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))\n",
        "tr_sen_a = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Cloudy Sensitivity']\n",
        "tr_sen_b = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Rain Sensitivity']\n",
        "tr_sen_c = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Shine Sensitivity']\n",
        "tr_sen_d = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sunrise Sensitivity']\n",
        "\n",
        "tr_spe_a = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Cloudy Specificity']\n",
        "tr_spe_b = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Rain Specificity']\n",
        "tr_spe_c = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Shine Specificity']\n",
        "tr_spe_d = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sunrise Specificity']\n",
        "\n",
        "tr_f1_a = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Cloudy F1-Score']\n",
        "tr_f1_b = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Rain F1-Score']\n",
        "tr_f1_c = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Shine F1-Score']\n",
        "tr_f1_d = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sunrise F1-Score']\n",
        "tr_bacc = balanced_accuracy_score(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))\n",
        "\n",
        "Training_Stats.append([tr_acc,\n",
        "                       tr_sen_a, tr_sen_b, tr_sen_c, tr_sen_d,\n",
        "                       tr_spe_a, tr_spe_b, tr_spe_c, tr_spe_d,\n",
        "                       tr_f1_a, tr_f1_b, tr_f1_c, tr_f1_d,\n",
        "                       tr_bacc])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nMkb1OAAjqa"
      },
      "source": [
        "Validation_Stats = []\n",
        "# Validation Measures\n",
        "\n",
        "v_acc = accuracy_score(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))\n",
        "v_sen_a = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Cloudy Sensitivity']\n",
        "v_sen_b = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Rain Sensitivity']\n",
        "v_sen_c = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Shine Sensitivity']\n",
        "v_sen_d = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sunrise Sensitivity']\n",
        "\n",
        "v_spe_a = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Cloudy Specificity']\n",
        "v_spe_b = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Rain Specificity']\n",
        "v_spe_c = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Shine Specificity']\n",
        "v_spe_d = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sunrise Specificity']\n",
        "\n",
        "v_f1_a = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Cloudy F1-Score']\n",
        "v_f1_b = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Rain F1-Score']\n",
        "v_f1_c = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Shine F1-Score']\n",
        "v_f1_d = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sunrise F1-Score']\n",
        "v_bacc = balanced_accuracy_score(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))\n",
        "\n",
        "Validation_Stats.append([v_acc,\n",
        "                  v_sen_a, v_sen_b, v_sen_c, v_sen_d,\n",
        "                  v_spe_a, v_spe_b, v_spe_c, v_spe_d,\n",
        "                  v_f1_a, v_f1_b, v_f1_c, v_f1_d,\n",
        "                  v_bacc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrRMPjLIA96T"
      },
      "source": [
        "Testing_Stats = []\n",
        "# Test Measures\n",
        "t_acc = accuracy_score(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))\n",
        "t_sen_a = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Cloudy Sensitivity']\n",
        "t_sen_b = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Rain Sensitivity']\n",
        "t_sen_c = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Shine Sensitivity']\n",
        "t_sen_d = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sunrise Sensitivity']\n",
        "\n",
        "t_spe_a = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Cloudy Specificity']\n",
        "t_spe_b = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Rain Specificity']\n",
        "t_spe_c = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Shine Specificity']\n",
        "t_spe_d = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sunrise Specificity']\n",
        "\n",
        "t_f1_a = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Cloudy F1-Score']\n",
        "t_f1_b = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Rain F1-Score']\n",
        "t_f1_c = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Shine F1-Score']\n",
        "t_f1_d = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sunrise F1-Score']\n",
        "t_bacc = balanced_accuracy_score(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))\n",
        "\n",
        "Testing_Stats.append([t_acc,\n",
        "                      t_sen_a, t_sen_b, t_sen_c, t_sen_d,\n",
        "                      t_spe_a, t_spe_b, t_spe_c, t_spe_d,\n",
        "                      t_f1_a, t_f1_b, t_f1_c, t_f1_d,\n",
        "                      t_bacc]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPTontB8qOt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d511cc-6f2e-4bcb-9bdf-62c65e85231f"
      },
      "source": [
        "Train_Statistics = np.asarray(Training_Stats)\n",
        "Val_Statistics = np.asarray(Validation_Stats)\n",
        "Test_Statistics = np.asarray(Testing_Stats)\n",
        "\n",
        "# Show Classification/Reconstruction Statistics\n",
        "Show_Statistics('Training Results', Train_Statistics[0])\n",
        "Show_Statistics('Validation Results', Val_Statistics[0])\n",
        "Show_Statistics('Test Results', Test_Statistics[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.9983333333333333\n",
            "Cloudy Sensitivity: 1.0\n",
            "Rain Sensitivity: 1.0\n",
            "Shine Sensitivity: 0.9933333333333333\n",
            "Sunrise Sensitivity: 1.0\n",
            "Cloudy Specificity: 0.9977777777777778\n",
            "Rain Specificity: 1.0\n",
            "Shine Specificity: 1.0\n",
            "Sunrise Specificity: 1.0\n",
            "Cloudy F1-Score: 0.9933774834437086\n",
            "Rain F1-Score: 1.0\n",
            "Shine F1-Score: 1.0\n",
            "Sunrise F1-Score: 1.0\n",
            "Balance Accuracy: 0.9983333333333333\n",
            "----------------------------------------------------------------------\n",
            "VALIDATION RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.9\n",
            "Cloudy Sensitivity: 0.8\n",
            "Rain Sensitivity: 0.95\n",
            "Shine Sensitivity: 0.9\n",
            "Sunrise Sensitivity: 0.95\n",
            "Cloudy Specificity: 0.9666666666666667\n",
            "Rain Specificity: 1.0\n",
            "Shine Specificity: 0.9333333333333333\n",
            "Sunrise Specificity: 0.9666666666666667\n",
            "Cloudy F1-Score: 0.7272727272727273\n",
            "Rain F1-Score: 0.95\n",
            "Shine F1-Score: 0.95\n",
            "Sunrise F1-Score: 0.8636363636363636\n",
            "Balance Accuracy: 0.8999999999999999\n",
            "----------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.9176136363636364\n",
            "Cloudy Sensitivity: 0.8378378378378378\n",
            "Rain Sensitivity: 1.0\n",
            "Shine Sensitivity: 0.8795180722891566\n",
            "Sunrise Sensitivity: 0.93048128342246\n",
            "Cloudy Specificity: 0.9428571428571428\n",
            "Rain Specificity: 0.993485342019544\n",
            "Shine Specificity: 0.966542750929368\n",
            "Sunrise Specificity: 1.0\n",
            "Cloudy F1-Score: 0.5636363636363636\n",
            "Rain F1-Score: 0.9574468085106383\n",
            "Shine F1-Score: 0.9574468085106383\n",
            "Sunrise F1-Score: 0.93048128342246\n",
            "Balance Accuracy: 0.9119592983873636\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}